import os
import mysql.connector
import pandas as pd
import pulp as pl
from datetime import datetime
from io import BytesIO
import boto3

# --- 1. Configuration (Environment Variables) ---
# This makes the script portable and easy to run in different environments (like Docker)
MYSQL_HOST = os.getenv("MYSQL_HOST", "mysql")
MYSQL_USER = os.getenv("MYSQL_USER", "root")
MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD", "root")
MYSQL_DATABASE = os.getenv("MYSQL_DATABASE", "court_db")
MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "http://minio:9000")
MINIO_ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY", "minioadmin")
MINIO_SECRET_KEY = os.getenv("MINIO_SECRET_KEY", "minioadmin")

# --- 2. The MILP Solver Logic ---
def run_milp_solver(df):
    """
    Implements a basic Mixed-Integer Linear Programming (MILP) model to optimize the court schedule.
    
    The model aims to maximize the total priority score of cases scheduled for the next day,
    subject to constraints like judge availability and time slot limitations.
    """
    print("ðŸ§  Running MILP solver to find optimal schedule...")

    # Define the optimization problem
    prob = pl.LpProblem("Court_Scheduling_Problem", pl.LpMaximize)

    # Define decision variables
    cases = df['case_id'].tolist()
    judges = ['Judge_A', 'Judge_B', 'Judge_C']
    
    # Let's assume a daily schedule has 5 time slots per judge, each 1-hour long
    time_slots = range(5) 

    # Binary variable x[i][j][k]: 1 if case i is assigned to judge j in time slot k, 0 otherwise
    x = pl.LpVariable.dicts("assign", (cases, judges, time_slots), 0, 1, pl.LpBinary)

    # Objective Function: Maximize the total priority score of all scheduled cases
    prob += pl.lpSum(
        df.loc[df['case_id'] == i, 'priority_score'].values[0] * x[i][j][k]
        for i in cases for j in judges for k in time_slots
    )

    # --- Constraints ---
    # 1. Each case must be scheduled at most once to prevent duplication
    for i in cases:
        prob += pl.lpSum(x[i][j][k] for j in judges for k in time_slots) <= 1, f"One_case_once_{i}"

    # 2. Each judge can handle only one case per time slot
    for j in judges:
        for k in time_slots:
            prob += pl.lpSum(x[i][j][k] for i in cases) <= 1, f"One_case_per_slot_{j}_{k}"
    
    # 3. Case duration must fit within a single time slot
    # This constraint assumes `estimated_duration_blocks` is in the same unit as `time_slots` (e.g., hours)
    for i in cases:
        for j in judges:
            for k in time_slots:
                prob += x[i][j][k] * df.loc[df['case_id'] == i, 'estimated_duration_blocks'].values[0] <= 1, f"Duration_fit_{i}_{j}_{k}"


    # Solve the problem
    prob.solve()

    print(f"Solver Status: {pl.LpStatus[prob.status]} ðŸš€")

    # Extract the optimized schedule from the solution
    schedule = []
    for i in cases:
        for j in judges:
            for k in time_slots:
                if pl.value(x[i][j][k]) == 1:
                    schedule.append({
                        'case_id': i,
                        'assigned_judge': j,
                        'time_slot': k,
                        'priority_score': df.loc[df['case_id'] == i, 'priority_score'].values[0]
                    })

    return pd.DataFrame(schedule)

# --- 3. Main Script Execution ---
def main():
    """
    Main function to orchestrate the pipeline:
    1. Pulls data from the database.
    2. Runs the MILP solver.
    3. Uploads the final schedule to MinIO.
    """
    conn = None
    try:
        # Connect to MySQL (the single source of truth)
        conn = mysql.connector.connect(
            host=MYSQL_HOST,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            database=MYSQL_DATABASE
        )
        query = "SELECT case_id, priority_score, estimated_duration_blocks FROM Case_Data WHERE status = 'filed'"
        df = pd.read_sql(query, conn)
        
        if df.empty:
            print("No cases to schedule. Exiting.")
            return

        print(f"Dataset ready. Pulled {len(df)} cases from the database.")
        
        # Run the MILP solver on the fresh data
        final_schedule = run_milp_solver(df)
        
        if final_schedule.empty:
            print("No schedule could be generated by the solver.")
            return

        # Upload the optimized schedule to MinIO
        s3 = boto3.client(
            's3',
            endpoint_url=MINIO_ENDPOINT,
            aws_access_key_id=MINIO_ACCESS_KEY,
            aws_secret_access_key=MINIO_SECRET_KEY
        )
        
        bucket_name = "optimized-schedules"
        try:
            # Check if the bucket exists, create it if not
            s3.head_bucket(Bucket=bucket_name)
        except Exception:
            s3.create_bucket(Bucket=bucket_name)

        output_file_name = f"optimized_schedule_{datetime.now().strftime('%Y%m%d')}.parquet"
        parquet_buffer = BytesIO()
        final_schedule.to_parquet(parquet_buffer, index=False)
        parquet_buffer.seek(0)

        s3.put_object(
            Bucket=bucket_name,
            Key=output_file_name,
            Body=parquet_buffer
        )
        
        print(f"âœ… Optimization complete. Saved definitive schedule to {bucket_name}/{output_file_name} in MinIO.")
        
    except Exception as e:
        print(f"An error occurred: {e}")
        # Optional: re-raise the exception if you want Airflow to mark the task as failed
        raise
    finally:
        if conn and conn.is_connected():
            conn.close()

if __name__ == "__main__":
    main()